{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from playwright.sync_api import sync_playwright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anime_season(month: str) -> str:\n",
    "    \"\"\"\n",
    "    This function converts a given month (as a string) into its corresponding season.\n",
    "\n",
    "    Parameters:\n",
    "    - month (str): A string representing the month in the format 'MM'. The valid values are '01' to '12'.\n",
    "\n",
    "    Returns:\n",
    "    - str: A string representing the season. The possible values are 'Winter', 'Spring', 'Summer', 'Fall', or 'Unspecified' if the input month is not within the range of 1 to 12.\n",
    "    \"\"\"\n",
    "    month_num = int(month)\n",
    "    seasons = [\"Winter\", \"Spring\", \"Summer\", \"Fall\"]\n",
    "    return seasons[(month_num - 1) // 3] if 1 <= month_num <= 12 else \"Unspecified\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_anime_data(anime_item) -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Scrape anime data from the HTML content.\n",
    "    \n",
    "    Args:\n",
    "    - anime_item (bs4.element.Tag): The BeautifulSoup object representing an anime item.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary containing scraped data about the anime.\n",
    "    \"\"\"\n",
    "    \n",
    "    title = anime_item.find('a', class_='link-title').text.strip()\n",
    "    \n",
    "    voters_text = anime_item.find('div', class_='scormem-item member')\n",
    "    voters = int(voters_text.text.strip().replace(',', '')) if voters_text else 'N/A'\n",
    "    \n",
    "    avg_score_text = anime_item.find('div', title='Score')\n",
    "    avg_score = float(avg_score_text.text.strip()) if avg_score_text else 'N/A'\n",
    "    \n",
    "    start_date_text = anime_item.find('span', class_='item')\n",
    "    start_date = start_date_text.text.strip() if start_date_text else 'N/A'\n",
    "    \n",
    "    status_text = anime_item.find('span', class_='item finished') or anime_item.find('span', class_='item airing')\n",
    "    status = status_text.text.strip() if status_text else 'N/A'\n",
    "    \n",
    "    studio_text = anime_item.find('span', class_='producer')\n",
    "    studio = studio_text.text.strip() if studio_text else 'N/A'\n",
    "    \n",
    "    genres = ', '.join([genre.text.strip() for genre in anime_item.find_all('span', class_='genre')]) if anime_item.find_all('span', class_='genre') else 'N/A'\n",
    "    \n",
    "    media_text = anime_item.find('span', class_='type')\n",
    "    media = media_text.text.strip() if media_text else 'N/A'\n",
    "\n",
    "    status_text = anime_item.find('span', class_='status')\n",
    "    status = status_text.text.strip() if status_text else 'N/A'\n",
    "\n",
    "    eps_text = anime_item.find('span', class_='eps')\n",
    "    eps = eps_text.text.strip().split()[0] if eps_text else 'N/A'\n",
    "\n",
    "    duration_text = anime_item.find('span', class_='duration')\n",
    "    duration = duration_text.text.strip().split()[0] if duration_text else 'N/A'\n",
    "    \n",
    "    synopsis_text = anime_item.find('p', class_='preline')\n",
    "    synopsis = synopsis_text.text.strip() if synopsis_text else 'N/A'\n",
    "    \n",
    "    return {\n",
    "        'Title': title,\n",
    "        'Voters': voters,\n",
    "        'Avg Score': avg_score,\n",
    "        'Start Date': start_date,\n",
    "        'Status': status,\n",
    "        'Studio': studio,\n",
    "        'Genres': genres,\n",
    "        'Media': media,\n",
    "        'Status': status,\n",
    "        'Eps': eps,\n",
    "        'Duration': duration,\n",
    "        'Synopsis': synopsis\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playwright_scraper(url: str, last: int) -> list[dict[str, str]]:\n",
    "    \"\"\"\n",
    "    This function uses Playwright to scrape anime data from a specified URL and its subsequent pages.\n",
    "    \n",
    "    -----\n",
    "    Parameters:\n",
    "    - url (str): The URL of the anime list to scrape.\n",
    "    - last (int): The last page number to scrape.\n",
    "    ----\n",
    "    Returns:\n",
    "    - list: A list of dictionaries, where each dictionary represents the scraped data of an anime.\n",
    "    ----\n",
    "    The function first launches a Chromium browser using Playwright and navigates to the specified URL. It then iterates through the pages from 1 to the specified last page, scraping anime data from each page. The scraped data is appended to a container list. Finally, the function closes the browser and returns the container list.\n",
    "    \"\"\"\n",
    "    container = []\n",
    "\n",
    "    with sync_playwright() as p:\n",
    "        try:\n",
    "            browser = p.chromium.launch(headless=True)  # Run in headless mode for efficiency\n",
    "            page = browser.new_page()\n",
    "            page.goto(url)\n",
    "\n",
    "            data_name = page.inner_text('.h1').split()[0]\n",
    "            print(f'Scraping data from {data_name}...')\n",
    "\n",
    "            # Use tqdm to display progress bar for page processing\n",
    "            for page_num in tqdm(range(1, last + 1), desc='Processing Pages', unit='page'):\n",
    "                page_url = f'{url}?page={page_num}'\n",
    "                \n",
    "                try:\n",
    "                    page.goto(page_url, wait_until='networkidle')\n",
    "                    if page.query_selector('.error404'):\n",
    "                        print(f'Page {page_num} of {data_name} does not exist.')\n",
    "                        break\n",
    "\n",
    "                    anime_list = page.query_selector_all('.js-anime-category-producer')\n",
    "                    for anime_item in anime_list:\n",
    "                        container.append(scrape_anime_data(anime_item))\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f'Error processing page {page_num}: {e}')\n",
    "                    break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error initializing Playwright: {e}')\n",
    "        \n",
    "        finally:\n",
    "            browser.close()\n",
    "\n",
    "    return container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeler(date: str, data: list[dict[str, str]]) -> None:\n",
    "    \"\"\"\n",
    "    Processes and saves anime data to a CSV file.\n",
    "\n",
    "    -----\n",
    "    Parameters:\n",
    "    - date (str): The date string used to name the CSV file.\n",
    "    - data (List[Dict[str, str]]): A list of dictionaries containing anime data.\n",
    "\n",
    "    -----\n",
    "    The function converts the list of dictionaries to a DataFrame, removes duplicate entries,\n",
    "    and saves the DataFrame to a CSV file in the 'data/processed' directory.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(data)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    file_path = f'data/processed/AnimeData_{date}.csv'\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f'Data saved to {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m all_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m url_list:\n\u001b[1;32m---> 28\u001b[0m     all_data\u001b[38;5;241m.\u001b[39mextend(\u001b[43mplaywright_scraper\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     30\u001b[0m modeler(date, all_data)\n",
      "Cell \u001b[1;32mIn[7], line 17\u001b[0m, in \u001b[0;36mplaywright_scraper\u001b[1;34m(url, last)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mThis function uses Playwright to scrape anime data from a specified URL and its subsequent pages.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03mThe function first launches a Chromium browser using Playwright and navigates to the specified URL. It then iterates through the pages from 1 to the specified last page, scraping anime data from each page. The scraped data is appended to a container list. Finally, the function closes the browser and returns the container list.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     15\u001b[0m container \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 17\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msync_playwright\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbrowser\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchromium\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheadless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Run in headless mode for efficiency\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\Asif Sayyed\\Documents\\GitHub\\AniMate\\venv\\Lib\\site-packages\\playwright\\sync_api\\_context_manager.py:47\u001b[0m, in \u001b[0;36mPlaywrightContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_own_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[1;32m---> 47\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Error(\n\u001b[0;32m     48\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"It looks like you are using Playwright Sync API inside the asyncio loop.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03mPlease use the Async API instead.\"\"\"\u001b[39;00m\n\u001b[0;32m     50\u001b[0m             )\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;66;03m# Create a new fiber for the protocol dispatcher. It will be pumping events\u001b[39;00m\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;66;03m# until the end of times. We will pass control to that fiber every time we\u001b[39;00m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;66;03m# block while waiting for a response.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgreenlet_main\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mError\u001b[0m: It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead."
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Preparation\n",
    "date = datetime.now().strftime('%d%m%y')\n",
    "\n",
    "# EXTRACT AND TRANSFORM\n",
    "url_list = [   \n",
    "    'https://myanimelist.net/anime/genre/1/',  # Action\n",
    "    'https://myanimelist.net/anime/genre/2/',  # Adventure\n",
    "    'https://myanimelist.net/anime/genre/5/',  # Avant Garde\n",
    "    'https://myanimelist.net/anime/genre/4/',  # Comedy\n",
    "    'https://myanimelist.net/anime/genre/8/',  # Drama\n",
    "    'https://myanimelist.net/anime/genre/10/', # Fantasy\n",
    "    'https://myanimelist.net/anime/genre/47/', # Gourmet\n",
    "    'https://myanimelist.net/anime/genre/14/', # Horror\n",
    "    'https://myanimelist.net/anime/genre/7/',  # Mystery\n",
    "    'https://myanimelist.net/anime/genre/22/', # Romance\n",
    "    'https://myanimelist.net/anime/genre/24/', # Sci-fi\n",
    "    'https://myanimelist.net/anime/genre/36/', # Slice-of-life\n",
    "    'https://myanimelist.net/anime/genre/30/', # Sport\n",
    "    'https://myanimelist.net/anime/genre/37/', # Supernatural\n",
    "    'https://myanimelist.net/anime/genre/41/'  # Suspense\n",
    "]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    all_data = []\n",
    "    for url in url_list:\n",
    "        all_data.extend(playwright_scraper(url, 100))\n",
    "    \n",
    "    modeler(date, all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
